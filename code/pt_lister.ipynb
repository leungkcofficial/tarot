{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Demographic dataframe\n",
    "## Coding below aims to get the reference code of all pateints recruited in this study\n",
    "- from January 1, 2009, to December 31, 2023\n",
    "- male and female \n",
    "- age > 18\n",
    "- estimated glomerular filtration rate (eGFR) of less than 60ml/min/1.73m2 according to the 2021 Chronic Kidney Disease Epidemiology Collaboration (CKD-EPI) formula\n",
    "\n",
    "## Original csv:\n",
    "- Reference key: unique for each patient\n",
    "- Clinic code: unique for each specialy clinic and admission, here only have MG (General Medicine) and GER (Geriatrics), codes related to admission, GOPD and non internal medicine related specialties have been deleted \n",
    "- Gender: M or F\n",
    "- Age: round to year\n",
    "- Date: Date of Creatinine investigation, format: YYYY-MM-DD\n",
    "- Creatinine: Round to integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the list of patients from serum creatinine investigation 2009 - 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2781568 entries, 0 to 2784967\n",
      "Data columns (total 7 columns):\n",
      " #   Column                                             Dtype  \n",
      "---  ------                                             -----  \n",
      " 0   Reference Key                                      float64\n",
      " 1   Date of Birth (yyyy-mm-dd)                         object \n",
      " 2   Sex                                                object \n",
      " 3   LIS Reference Datetime                             object \n",
      " 4   LIS Case No.                                       object \n",
      " 5   LIS Result (28 days) - LIS Result: Numeric Result  float64\n",
      " 6                                                      object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 169.8+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/mnt/d/pydatascience/g3_regress/data/Cr'\n",
    "file_pattern = f\"{path}/RRT*.csv\"\n",
    "dataframes = [pd.read_csv(filename) for filename in glob.glob(file_pattern)]\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "# Drop rows where 'LIS Result (28 days) - LIS Result: Numeric Result' is missing\n",
    "combined_df = combined_df.dropna(subset=['LIS Result (28 days) - LIS Result: Numeric Result'])\n",
    "# Drop rows where both 'Sex' and 'Reference Key' are missing\n",
    "combined_df = combined_df.dropna(subset=['Sex', 'Reference Key'], how='all')\n",
    "# Create a DataFrame with unique 'Reference Key' and non-null 'Sex'\n",
    "reference_df = combined_df.dropna(subset=['Sex']).drop_duplicates(subset=['Reference Key'])\n",
    "reference_df = reference_df[['Reference Key', 'Sex']]\n",
    "# Merge the original DataFrame with this reference DataFrame on 'Reference Key' to fill missing 'Sex'\n",
    "# This uses a left join to ensure all original rows in combined_df are preserved\n",
    "combined_df = combined_df.merge(reference_df, on='Reference Key', suffixes=('', '_filled'), how='left')\n",
    "# Fill missing 'Sex' values using the 'Sex_filled' values\n",
    "combined_df['Sex'] = combined_df['Sex'].fillna(combined_df['Sex_filled'])\n",
    "# Now that 'Sex' is filled, drop the auxiliary 'Sex_filled' column\n",
    "combined_df.drop(columns='Sex_filled', inplace=True)\n",
    "# Drop rows where both 'Sex' and 'Date of Birth (yyyy-mm-dd)' are missing\n",
    "combined_df = combined_df.dropna(subset=['Sex', 'Date of Birth (yyyy-mm-dd)'])\n",
    "combined_df.info()\n",
    "del [reference_df, dataframes, file_pattern, path]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve patient demographics dataframe\n",
    "- key: unique for each patient\n",
    "- dob: Date of birth\n",
    "- gender: M or F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate each unique Reference Key with the corresponding Date of Birth, and Gender\n",
    "demographic_df = combined_df.drop_duplicates(subset=['Reference Key','Sex', 'Date of Birth (yyyy-mm-dd)'])\n",
    "demographic_df = demographic_df[['Reference Key', 'Date of Birth (yyyy-mm-dd)', 'Sex']].rename(\n",
    "    columns={'Reference Key': 'key', 'Date of Birth (yyyy-mm-dd)': 'dob', 'Sex': 'gender'}\n",
    ")\n",
    "demographic_df['dob'] = pd.to_datetime(demographic_df['dob'], format='mixed')\n",
    "demographic_df = demographic_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Creatinine dataframe\n",
    "- key: unique for each patient\n",
    "- date: date and time of investigation\n",
    "- code: clinic or in-patient code of investigation\n",
    "- Cr: numerical level of serum creatinine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr_df = combined_df[['Reference Key', 'LIS Reference Datetime', 'LIS Case No.', 'LIS Result (28 days) - LIS Result: Numeric Result']].rename(\n",
    "    columns={'Reference Key': 'key',\n",
    "             'LIS Reference Datetime': 'date',\n",
    "             'LIS Case No.': 'code', \n",
    "             'LIS Result (28 days) - LIS Result: Numeric Result': 'Cr'}\n",
    ")\n",
    "cr_df['date'] = pd.to_datetime(cr_df['date'], format='mixed')\n",
    "del combined_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate eGFR according to 2021 race free CKD-EPI equation\n",
    "- Inker Lesley A., Eneanya Nwamaka D., Coresh Josef, Tighiouart Hocine, Wang Dan, Sang Yingying, et al. New Creatinine- and Cystatin C–Based Equations to Estimate GFR without Race. New England Journal of Medicine. 2021;385(19): 1737–1749. https://doi.org/10.1056/NEJMoa2102953.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>date</th>\n",
       "      <th>code</th>\n",
       "      <th>Cr</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>eGFRcr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>449.0</td>\n",
       "      <td>2009-04-08 11:12:00</td>\n",
       "      <td>HN08019759M</td>\n",
       "      <td>125.0</td>\n",
       "      <td>1930-02-22</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>50.479592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>449.0</td>\n",
       "      <td>2009-06-22 09:56:00</td>\n",
       "      <td>HN09012526S</td>\n",
       "      <td>108.0</td>\n",
       "      <td>1930-02-22</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>60.082047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>449.0</td>\n",
       "      <td>2009-06-30 09:03:00</td>\n",
       "      <td>MG0910744O</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1930-02-22</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>52.939103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>449.0</td>\n",
       "      <td>2009-07-07 15:03:00</td>\n",
       "      <td>HN09013776M</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1930-02-22</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>55.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>449.0</td>\n",
       "      <td>2009-07-08 11:04:00</td>\n",
       "      <td>HN090656875</td>\n",
       "      <td>129.0</td>\n",
       "      <td>1930-02-22</td>\n",
       "      <td>M</td>\n",
       "      <td>79</td>\n",
       "      <td>48.531896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     key                date         code     Cr        dob gender  age  \\\n",
       "0  449.0 2009-04-08 11:12:00  HN08019759M  125.0 1930-02-22      M   79   \n",
       "2  449.0 2009-06-22 09:56:00  HN09012526S  108.0 1930-02-22      M   79   \n",
       "3  449.0 2009-06-30 09:03:00   MG0910744O  120.0 1930-02-22      M   79   \n",
       "4  449.0 2009-07-07 15:03:00  HN09013776M  116.0 1930-02-22      M   79   \n",
       "5  449.0 2009-07-08 11:04:00  HN090656875  129.0 1930-02-22      M   79   \n",
       "\n",
       "      eGFRcr  \n",
       "0  50.479592  \n",
       "2  60.082047  \n",
       "3  52.939103  \n",
       "4  55.130600  \n",
       "5  48.531896  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge cr_df with demographic_df on the 'key' column\n",
    "egfr_df = pd.merge(cr_df, demographic_df, on='key', how ='outer')\n",
    "egfr_df = egfr_df.drop_duplicates()\n",
    "# Convert creatinine from µmol/L to mg/dL\n",
    "egfr_df['Cr_mg_dL'] = egfr_df['Cr'] / 88.4\n",
    "\n",
    "# Calculate age in years at the time of the creatinine measurement\n",
    "egfr_df['age'] = (egfr_df['date'] - egfr_df['dob']).dt.days / 365.25\n",
    "\n",
    "# Constants based on sex\n",
    "conditions = [\n",
    "    egfr_df['gender'] == 'F',\n",
    "    egfr_df['gender'] == 'M'\n",
    "]\n",
    "\n",
    "choices_k = [0.7, 0.9]  # kappa values for female and male\n",
    "choices_alpha = [-0.241, -0.302]  # alpha values for female and male\n",
    "egfr_df['kappa'] = np.select(conditions, choices_k, default=np.nan)\n",
    "egfr_df['alpha'] = np.select(conditions, choices_alpha, default=np.nan)\n",
    "\n",
    "# Calculate eGFR using the CKD-EPI 2021 equation\n",
    "egfr_df['eGFRcr'] = 142 * (egfr_df['Cr_mg_dL'] / egfr_df['kappa']).clip(upper=1)**egfr_df['alpha'] * \\\n",
    "                        (egfr_df['Cr_mg_dL'] / egfr_df['kappa']).clip(lower=1)**(-1.2) * \\\n",
    "                        0.9938**egfr_df['age']\n",
    "\n",
    "# Adjust eGFR for females\n",
    "egfr_df.loc[egfr_df['gender'] == 'F', 'eGFRcr'] *= 1.012\n",
    "# Clean the 'code' column by removing '?'\n",
    "egfr_df['code'] = egfr_df['code'].str.replace('?', '', regex=False)\n",
    "# Sort the DataFrame by 'key', 'code', and 'date' to prepare for filtering\n",
    "egfr_df_sorted = egfr_df.sort_values(by=['key', 'code', 'date'])\n",
    "# Create a flag to identify 'RRT' codes, convert to boolean and fill NaN\n",
    "egfr_df_sorted['is_rrt'] = egfr_df_sorted['code'].str.startswith('RRT').astype(bool).fillna(False)\n",
    "# Create a flag to identify 'HN' codes, convert to boolean and fill NaN\n",
    "egfr_df_sorted['is_hn'] = egfr_df_sorted['code'].str.startswith('HN').astype(bool).fillna(False)\n",
    "# Mark the last row in each group of 'key' and 'code'\n",
    "egfr_df_sorted['last_in_group'] = egfr_df_sorted['date'] == egfr_df_sorted.groupby(['key', 'code'])['date'].transform('max')\n",
    "egfr_df_sorted['last_in_group'] = egfr_df_sorted['last_in_group'].astype(bool).fillna(False)\n",
    "# Filter rows: Exclude rows where the code starts with 'RRT', and for 'HN' codes, keep only the last in the group\n",
    "egfr_df_filtered = egfr_df_sorted[~(egfr_df_sorted['is_hn'] & ~egfr_df_sorted['last_in_group']) & ~egfr_df_sorted['is_rrt']]\n",
    "# Drop the columns used for filtering to clean up the DataFrame\n",
    "egfr_df_final = egfr_df_filtered.drop(['is_hn', 'last_in_group', 'is_rrt'], axis=1)\n",
    "# Final DataFrame sorted by 'key' and 'date'\n",
    "egfr_df = egfr_df_final.sort_values(by=['key', 'date'])\n",
    "# Calculate age in years\n",
    "egfr_df['age'] = (egfr_df['date'] - egfr_df['dob']).dt.days / 365.25\n",
    "egfr_df['age'] = egfr_df['age'].round().astype(int)\n",
    "# Filter out rows where 'age' is less than 18\n",
    "egfr_df = egfr_df[egfr_df['age'] >= 18]\n",
    "# Count the number of entries for each 'key'\n",
    "egfr_df['entry_count'] = egfr_df.groupby('key')['key'].transform('count')\n",
    "# Filter out rows where the 'key' has only one entry\n",
    "egfr_df = egfr_df[egfr_df['entry_count'] > 1]\n",
    "# Drop the 'entry_count' column as it's no longer needed after filtering\n",
    "egfr_df.drop(columns=['entry_count', 'Cr_mg_dL', 'kappa', 'alpha'], inplace=True)\n",
    "\n",
    "# Display the results\n",
    "display(egfr_df.head())\n",
    "\n",
    "del [egfr_df_sorted, egfr_df_final, egfr_df_filtered, choices_alpha, choices_k, conditions]\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve list of reference key, date of first diagnosed CKD with eGFR less than starting point (now we use 60) and date of first diagnosed with eGFR less than end point (now we use 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_persistent_low_egfr(df, threshold, days):\n",
    "    # Ensure the DataFrame is sorted by 'key' and 'date'\n",
    "    df = df.sort_values(by=['key', 'date'])\n",
    "    column_name = f'first_sub_{threshold}_date'\n",
    "    # Initialize a column to store the valid end date when conditions are met\n",
    "    df[column_name] = pd.NaT\n",
    "\n",
    "    # Iterate over each unique key\n",
    "    for key, group in df.groupby('key'):\n",
    "        i = 0\n",
    "        while i < len(group):\n",
    "            row = group.iloc[i]\n",
    "            if row['eGFRcr'] < threshold:\n",
    "                # Start the check from this row\n",
    "                j = i\n",
    "                valid_end_date = None\n",
    "                # Continue to find a valid range\n",
    "                while j < len(group):\n",
    "                    # Check if the day difference meets the requirement\n",
    "                    if (group.iloc[j]['date'] - row['date']).days > days:\n",
    "                        # Calculate mean eGFRcr over this range\n",
    "                        mean_egfr = group.iloc[i:j+1]['eGFRcr'].mean()\n",
    "                        if mean_egfr < threshold:\n",
    "                            valid_end_date = group.iloc[j]['date']\n",
    "                            break  # End date found, break the inner loop\n",
    "                        else:\n",
    "                            break  # Mean eGFR not valid, break the inner loop\n",
    "                    j += 1\n",
    "                if valid_end_date:\n",
    "                    df.loc[(df['key'] == key) & (df['date'] == valid_end_date), column_name] = valid_end_date\n",
    "                    break  # Move to the next key since a valid period is found\n",
    "            i += 1\n",
    "\n",
    "    return df\n",
    "\n",
    "sub_60_df = find_persistent_low_egfr(egfr_df, threshold=60, days=90)\n",
    "sub_10_df = find_persistent_low_egfr(egfr_df, threshold=10, days=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of patients: 39048\n",
      "Excluded number of patients who present with eGFR < 10ml/min/1.73m2 (crash landers): 1132\n",
      "Number of patients after excluding crash lander: 37916\n",
      "Total number of patients developed eGFRcr < 10ml/min/1.73m2: 3614\n",
      "Total number of new patients after 2019-01-01 as test set: 6066\n",
      "Total number of patients developed eGFRcr < 10ml/min/1.73m2 in test set: 239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_ls = sub_60_df[['key', 'first_sub_60_date']].dropna().reset_index().drop_duplicates()\n",
    "pt_ls_10 = sub_10_df[['key', 'first_sub_10_date']].dropna().reset_index().drop_duplicates()\n",
    "pt_ls = pt_ls.merge(pt_ls_10, on='key', how='left').drop(columns=['index_x', 'index_y']).drop_duplicates()\n",
    "crash_lander_ls = pt_ls[(pt_ls['first_sub_60_date'] >= pt_ls['first_sub_10_date'])]\n",
    "print('Total number of patients:', pt_ls['key'].nunique())\n",
    "print('Excluded number of patients who present with eGFR < 10ml/min/1.73m2 (crash landers):', crash_lander_ls['key'].nunique())\n",
    "pt_ls = pt_ls[~pt_ls['key'].isin(crash_lander_ls['key'].unique())]\n",
    "print('Number of patients after excluding crash lander:', pt_ls['key'].nunique())\n",
    "print('Total number of patients developed eGFRcr < 10ml/min/1.73m2:', pt_ls['first_sub_10_date'].notnull().sum())\n",
    "pt_ls.to_csv('/mnt/d/pydatascience/g3_regress/data/pt_ls.csv', index=False)\n",
    "min_dates = egfr_df.groupby('key')['date'].min()\n",
    "keys_after_2019 = min_dates[min_dates > '2019-01-01'].index\n",
    "test_ls = pt_ls[pt_ls['key'].isin(keys_after_2019)]\n",
    "test_ls.to_csv('/mnt/d/pydatascience/g3_regress/data/test_ls.csv', index=False)\n",
    "print('Total number of new patients after 2019-01-01 as test set:', test_ls['key'].nunique())\n",
    "print('Total number of patients developed eGFRcr < 10ml/min/1.73m2 in test set:', test_ls['first_sub_10_date'].notnull().sum())\n",
    "\n",
    "del [pt_ls_10]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_df.to_csv('/mnt/d/pydatascience/g3_regress/data/demographic_df.csv', index=False)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_1.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_2.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_3.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_4.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_5.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_6.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_7.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_8.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_9.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_10.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_11.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_12.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_13.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_14.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_15.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_16.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_17.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_18.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_19.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_20.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_21.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_22.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_23.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_24.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_25.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_26.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_27.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_28.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_29.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_30.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_31.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_32.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_33.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_34.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_35.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_36.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_37.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_38.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_39.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_40.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_41.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_42.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_43.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_44.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_45.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_46.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_47.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_48.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_49.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_50.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_51.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_52.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_53.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_54.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_55.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_56.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_57.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_58.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_59.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_60.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_61.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_62.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_63.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_64.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_65.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_66.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_67.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_68.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_69.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_70.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_71.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_72.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_73.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_74.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_75.txt\n",
      "Saved to /mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_76.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_and_impute_dates(date_series):\n",
    "    default_date = datetime.strptime('31/12/2023 23:59:59', '%d/%m/%Y %H:%M:%S')\n",
    "    date_series = date_series.fillna(default_date)\n",
    "\n",
    "    # Convert to string format \"yyyy/mm/dd\"\n",
    "    return date_series.dt.strftime('%Y/%m/%d')\n",
    "\n",
    "# Apply formatting and imputation\n",
    "pt_ls_2 = pt_ls.copy()\n",
    "pt_ls_2['first_sub_60_date'] = format_and_impute_dates(pt_ls_2['first_sub_60_date'])\n",
    "pt_ls_2['first_sub_10_date'] = format_and_impute_dates(pt_ls_2['first_sub_10_date'])\n",
    "pt_ls_2['key'] = pt_ls_2['key'].astype(int)\n",
    "\n",
    "\n",
    "# Function to split DataFrame and save to files\n",
    "def save_to_files(df, max_rows=500):\n",
    "    # Calculate number of files needed\n",
    "    num_files = (len(df) + max_rows - 1) // max_rows\n",
    "    \n",
    "    # Loop to create each file\n",
    "    for i in range(num_files):\n",
    "        # Slice the DataFrame\n",
    "        start_idx = i * max_rows\n",
    "        end_idx = start_idx + max_rows\n",
    "        df_slice = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # File path\n",
    "        file_path = f'/mnt/d/pydatascience/g3_regress/data/pt_ls/fine_output_keys_{i+1}.txt'\n",
    "        \n",
    "        # Save to text file with tab delimiter\n",
    "        df_slice.to_csv(file_path, sep='\\t', index=False, header=False)\n",
    "        print(f\"Saved to {file_path}\")\n",
    "\n",
    "# Apply the function to save files\n",
    "save_to_files(pt_ls_2)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15567/2214653394.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  keys_df['key'] = keys_df['key'].astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to keys_chunk_1.txt\n",
      "Saved to keys_chunk_2.txt\n",
      "Saved to keys_chunk_3.txt\n",
      "Saved to keys_chunk_4.txt\n",
      "Saved to keys_chunk_5.txt\n",
      "Saved to keys_chunk_6.txt\n",
      "Saved to keys_chunk_7.txt\n",
      "Saved to keys_chunk_8.txt\n",
      "Saved to keys_chunk_9.txt\n",
      "Saved to keys_chunk_10.txt\n",
      "Saved to keys_chunk_11.txt\n",
      "Saved to keys_chunk_12.txt\n",
      "Saved to keys_chunk_13.txt\n",
      "Saved to keys_chunk_14.txt\n",
      "Saved to keys_chunk_15.txt\n",
      "Saved to keys_chunk_16.txt\n",
      "Saved to keys_chunk_17.txt\n",
      "Saved to keys_chunk_18.txt\n",
      "Saved to keys_chunk_19.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_keys_in_chunks(df, max_keys=2000):\n",
    "    # Ensure the DataFrame only contains the 'key' column\n",
    "    keys_df = df[['key']]\n",
    "\n",
    "    # Convert 'key' to integers\n",
    "    keys_df['key'] = keys_df['key'].astype(int)\n",
    "\n",
    "    # Calculate the number of chunks needed\n",
    "    num_chunks = (len(keys_df) + max_keys - 1) // max_keys\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        # Slice the DataFrame\n",
    "        start_idx = i * max_keys\n",
    "        end_idx = start_idx + max_keys\n",
    "        chunk = keys_df.iloc[start_idx:end_idx]\n",
    "\n",
    "        # File path\n",
    "        file_path = f'keys_chunk_{i+1}.txt'\n",
    "\n",
    "        # Save to a tab-delimited text file without header\n",
    "        chunk.to_csv(file_path, sep='\\t', index=False, header=False)\n",
    "        print(f\"Saved to {file_path}\")\n",
    "\n",
    "# Assuming pt_ls_2 is already defined\n",
    "save_keys_in_chunks(pt_ls_2)\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "g3_regress",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
